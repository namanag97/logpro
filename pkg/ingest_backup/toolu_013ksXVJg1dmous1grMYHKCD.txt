     1→package ingest
     2→
     3→import (
     4→	"bufio"
     5→	"bytes"
     6→	"compress/gzip"
     7→	"context"
     8→	"encoding/json"
     9→	"fmt"
    10→	"io"
    11→	"os"
    12→	"path/filepath"
    13→	"strconv"
    14→	"sync"
    15→	"sync/atomic"
    16→	"time"
    17→	"unicode/utf8"
    18→
    19→	"github.com/apache/arrow/go/v14/arrow"
    20→	"github.com/apache/arrow/go/v14/arrow/array"
    21→	"github.com/apache/arrow/go/v14/arrow/memory"
    22→	"github.com/apache/arrow/go/v14/parquet"
    23→	"github.com/apache/arrow/go/v14/parquet/compress"
    24→	"github.com/apache/arrow/go/v14/parquet/pqarrow"
    25→)
    26→
    27→// RobustPath handles messy data with error recovery and validation.
    28→type RobustPath struct {
    29→	// Buffer pools for zero-allocation parsing
    30→	bufferPool  sync.Pool
    31→	recordPool  sync.Pool
    32→}
    33→
    34→// NewRobustPath creates a new robust processing path.
    35→func NewRobustPath() *RobustPath {
    36→	return &RobustPath{
    37→		bufferPool: sync.Pool{
    38→			New: func() interface{} {
    39→				return make([]byte, 0, 64*1024) // 64KB buffers
    40→			},
    41→		},
    42→		recordPool: sync.Pool{
    43→			New: func() interface{} {
    44→				return &Record{
    45→					Fields: make([][]byte, 0, 32),
    46→				}
    47→			},
    48→		},
    49→	}
    50→}
    51→
    52→// Record represents a parsed row with error tracking.
    53→type Record struct {
    54→	Fields      [][]byte
    55→	LineNumber  int64
    56→	ByteOffset  int64
    57→	ParseError  error
    58→	Recovered   bool
    59→}
    60→
    61→// Reset clears the record for reuse.
    62→func (r *Record) Reset() {
    63→	r.Fields = r.Fields[:0]
    64→	r.LineNumber = 0
    65→	r.ByteOffset = 0
    66→	r.ParseError = nil
    67→	r.Recovered = false
    68→}
    69→
    70→// Process handles file conversion with robust error handling.
    71→func (r *RobustPath) Process(ctx context.Context, inputPath, outputPath string, analysis *FileAnalysis, opts Options) (*Result, error) {
    72→	// Ensure output directory exists
    73→	if err := os.MkdirAll(filepath.Dir(outputPath), 0755); err != nil {
    74→		return nil, fmt.Errorf("failed to create output directory: %w", err)
    75→	}
    76→
    77→	switch analysis.Format {
    78→	case FormatCSV, FormatTSV:
    79→		return r.processCSV(ctx, inputPath, outputPath, analysis, opts)
    80→	case FormatJSON, FormatJSONL:
    81→		return r.processJSON(ctx, inputPath, outputPath, analysis, opts)
    82→	case FormatXES:
    83→		return r.processXES(ctx, inputPath, outputPath, analysis, opts)
    84→	case FormatGzip:
    85→		return r.processGzip(ctx, inputPath, outputPath, analysis, opts)
    86→	default:
    87→		return nil, fmt.Errorf("format %s not supported by robust path", analysis.Format)
    88→	}
    89→}
    90→
    91→// processCSV converts CSV with robust error handling.
    92→func (r *RobustPath) processCSV(ctx context.Context, inputPath, outputPath string, analysis *FileAnalysis, opts Options) (*Result, error) {
    93→	// Open input file
    94→	f, err := os.Open(inputPath)
    95→	if err != nil {
    96→		return nil, fmt.Errorf("failed to open input: %w", err)
    97→	}
    98→	defer f.Close()
    99→
   100→	inputInfo, _ := f.Stat()
   101→
   102→	// Create buffered reader
   103→	bufSize := opts.BufferSize
   104→	if bufSize <= 0 {
   105→		bufSize = 256 * 1024
   106→	}
   107→	reader := bufio.NewReaderSize(f, bufSize)
   108→
   109→	// Parse header
   110→	headerLine, err := r.readLine(reader)
   111→	if err != nil {
   112→		return nil, fmt.Errorf("failed to read header: %w", err)
   113→	}
   114→
   115→	headers := r.parseCSVLine(headerLine, analysis.Delimiter, analysis.QuoteChar)
   116→	if len(headers) == 0 {
   117→		return nil, fmt.Errorf("no headers found")
   118→	}
   119→
   120→	// Infer types from sample
   121→	types := r.inferTypes(reader, analysis, headers, 1000)
   122→
   123→	// Reset reader (reopen file)
   124→	f.Seek(0, 0)
   125→	reader = bufio.NewReaderSize(f, bufSize)
   126→	reader.ReadBytes('\n') // Skip header
   127→
   128→	// Create Arrow schema
   129→	schema := r.buildArrowSchema(headers, types)
   130→
   131→	// Create Parquet writer
   132→	outFile, err := os.Create(outputPath)
   133→	if err != nil {
   134→		return nil, fmt.Errorf("failed to create output: %w", err)
   135→	}
   136→	defer outFile.Close()
   137→
   138→	writerProps := r.buildWriterProps(opts)
   139→	arrowProps := pqarrow.NewArrowWriterProperties(pqarrow.WithAllocator(memory.DefaultAllocator))
   140→
   141→	writer, err := pqarrow.NewFileWriter(schema, outFile, writerProps, arrowProps)
   142→	if err != nil {
   143→		return nil, fmt.Errorf("failed to create writer: %w", err)
   144→	}
   145→	defer writer.Close()
   146→
   147→	// Process records in batches
   148→	batchSize := 8192
   149→	builders := r.createBuilders(schema, memory.DefaultAllocator)
   150→
   151→	var rowCount int64
   152→	var errorCount int64
   153→	var recoveredCount int64
   154→	lineNum := int64(1)
   155→
   156→	for {
   157→		select {
   158→		case <-ctx.Done():
   159→			return nil, ctx.Err()
   160→		default:
   161→		}
   162→
   163→		line, err := r.readLine(reader)
   164→		if err == io.EOF {
   165→			break
   166→		}
   167→		if err != nil {
   168→			return nil, fmt.Errorf("read error at line %d: %w", lineNum, err)
   169→		}
   170→
   171→		lineNum++
   172→
   173→		// Parse with error recovery
   174→		fields, parseErr, recovered := r.parseCSVLineRobust(line, analysis.Delimiter, analysis.QuoteChar, len(headers))
   175→
   176→		if parseErr != nil && !recovered {
   177→			errorCount++
   178→			if opts.MaxErrors > 0 && errorCount >= int64(opts.MaxErrors) {
   179→				return nil, fmt.Errorf("too many errors (%d), aborting", errorCount)
   180→			}
   181→			continue
   182→		}
   183→
   184→		if recovered {
   185→			recoveredCount++
   186→		}
   187→
   188→		// Add to builders
   189→		r.appendRecord(builders, types, fields, headers)
   190→		rowCount++
   191→
   192→		// Flush batch if needed
   193→		if rowCount > 0 && rowCount%int64(batchSize) == 0 {
   194→			if err := r.flushBatch(writer, schema, builders); err != nil {
   195→				return nil, fmt.Errorf("failed to write batch: %w", err)
   196→			}
   197→		}
   198→	}
   199→
   200→	// Flush remaining
   201→	if rowCount%int64(batchSize) != 0 {
   202→		if err := r.flushBatch(writer, schema, builders); err != nil {
   203→			return nil, fmt.Errorf("failed to write final batch: %w", err)
   204→		}
   205→	}
   206→
   207→	// Get output size
   208→	outputInfo, _ := os.Stat(outputPath)
   209→	outputSize := int64(0)
   210→	if outputInfo != nil {
   211→		outputSize = outputInfo.Size()
   212→	}
   213→
   214→	result := &Result{
   215→		InputPath:       inputPath,
   216→		OutputPath:      outputPath,
   217→		Format:          analysis.Format,
   218→		Strategy:        StrategyRobustGo,
   219→		RowCount:        rowCount,
   220→		ColumnCount:     len(headers),
   221→		InputSize:       inputInfo.Size(),
   222→		OutputSize:      outputSize,
   223→		CompressionRate: float64(inputInfo.Size()) / float64(outputSize),
   224→		Quality: &QualityMetrics{
   225→			MalformedRows: errorCount,
   226→			RecoveredRows: recoveredCount,
   227→		},
   228→	}
   229→
   230→	return result, nil
   231→}
   232→
   233→// readLine reads a line handling embedded newlines in quotes.
   234→func (r *RobustPath) readLine(reader *bufio.Reader) ([]byte, error) {
   235→	var line []byte
   236→	inQuote := false
   237→
   238→	for {
   239→		part, err := reader.ReadBytes('\n')
   240→		if len(part) > 0 {
   241→			line = append(line, part...)
   242→
   243→			// Count quotes to track state
   244→			for _, b := range part {
   245→				if b == '"' {
   246→					inQuote = !inQuote
   247→				}
   248→			}
   249→
   250→			// If we're not in a quote, line is complete
   251→			if !inQuote {
   252→				return bytes.TrimRight(line, "\r\n"), nil
   253→			}
   254→		}
   255→
   256→		if err != nil {
   257→			if err == io.EOF && len(line) > 0 {
   258→				return bytes.TrimRight(line, "\r\n"), nil
   259→			}
   260→			return line, err
   261→		}
   262→	}
   263→}
   264→
   265→// parseCSVLine parses a CSV line into fields.
   266→func (r *RobustPath) parseCSVLine(line []byte, delim, quote byte) [][]byte {
   267→	var fields [][]byte
   268→	var field []byte
   269→	inQuote := false
   270→
   271→	for i := 0; i < len(line); i++ {
   272→		b := line[i]
   273→
   274→		if b == quote {
   275→			if inQuote && i+1 < len(line) && line[i+1] == quote {
   276→				// Escaped quote
   277→				field = append(field, quote)
   278→				i++
   279→			} else {
   280→				inQuote = !inQuote
   281→			}
   282→		} else if b == delim && !inQuote {
   283→			fields = append(fields, field)
   284→			field = nil
   285→		} else {
   286→			field = append(field, b)
   287→		}
   288→	}
   289→
   290→	fields = append(fields, field)
   291→	return fields
   292→}
   293→
   294→// parseCSVLineRobust parses with error recovery.
   295→func (r *RobustPath) parseCSVLineRobust(line []byte, delim, quote byte, expectedCols int) ([][]byte, error, bool) {
   296→	fields := r.parseCSVLine(line, delim, quote)
   297→	recovered := false
   298→
   299→	// Handle ragged rows
   300→	if len(fields) < expectedCols {
   301→		// Pad with empty fields
   302→		for len(fields) < expectedCols {
   303→			fields = append(fields, nil)
   304→		}
   305→		recovered = true
   306→	} else if len(fields) > expectedCols {
   307→		// Truncate (but keep note)
   308→		fields = fields[:expectedCols]
   309→		recovered = true
   310→	}
   311→
   312→	// Check for encoding issues
   313→	for i, f := range fields {
   314→		if !utf8.Valid(f) {
   315→			// Replace invalid UTF-8 with replacement character
   316→			fields[i] = bytes.ToValidUTF8(f, []byte("\uFFFD"))
   317→			recovered = true
   318→		}
   319→	}
   320→
   321→	return fields, nil, recovered
   322→}
   323→
   324→// inferTypes samples rows to determine column types.
   325→func (r *RobustPath) inferTypes(reader *bufio.Reader, analysis *FileAnalysis, headers [][]byte, sampleSize int) []ColumnType {
   326→	types := make([]ColumnType, len(headers))
   327→	for i := range types {
   328→		types[i] = TypeUnknown
   329→	}
   330→
   331→	counts := make([]map[ColumnType]int, len(headers))
   332→	for i := range counts {
   333→		counts[i] = make(map[ColumnType]int)
   334→	}
   335→
   336→	for i := 0; i < sampleSize; i++ {
   337→		line, err := r.readLine(reader)
   338→		if err != nil {
   339→			break
   340→		}
   341→
   342→		fields := r.parseCSVLine(line, analysis.Delimiter, analysis.QuoteChar)
   343→
   344→		for j, field := range fields {
   345→			if j >= len(counts) {
   346→				break
   347→			}
   348→			t := r.detectFieldType(field)
   349→			counts[j][t]++
   350→		}
   351→	}
   352→
   353→	// Select most common type per column
   354→	for i, countMap := range counts {
   355→		maxCount := 0
   356→		for t, count := range countMap {
   357→			if count > maxCount && t != TypeNull {
   358→				maxCount = count
   359→				types[i] = t
   360→			}
   361→		}
   362→		if types[i] == TypeUnknown {
   363→			types[i] = TypeString // Default to string
   364→		}
   365→	}
   366→
   367→	return types
   368→}
   369→
   370→// ColumnType represents inferred column type.
   371→type ColumnType uint8
   372→
   373→const (
   374→	TypeUnknown ColumnType = iota
   375→	TypeNull
   376→	TypeString
   377→	TypeInt64
   378→	TypeFloat64
   379→	TypeBool
   380→	TypeTimestamp
   381→)
   382→
   383→// detectFieldType determines the type of a field value.
   384→func (r *RobustPath) detectFieldType(value []byte) ColumnType {
   385→	if len(value) == 0 {
   386→		return TypeNull
   387→	}
   388→
   389→	s := string(bytes.TrimSpace(value))
   390→
   391→	// Check for null values
   392→	switch s {
   393→	case "", "NULL", "null", "NA", "N/A", "n/a", "None", "none", "nil", "-":
   394→		return TypeNull
   395→	}
   396→
   397→	// Check for boolean
   398→	switch s {
   399→	case "true", "false", "True", "False", "TRUE", "FALSE", "1", "0", "yes", "no":
   400→		return TypeBool
   401→	}
   402→
   403→	// Check for integer
   404→	if _, err := strconv.ParseInt(s, 10, 64); err == nil {
   405→		return TypeInt64
   406→	}
   407→
   408→	// Check for float
   409→	if _, err := strconv.ParseFloat(s, 64); err == nil {
   410→		return TypeFloat64
   411→	}
   412→
   413→	// Check for timestamp (common formats)
   414→	for _, layout := range []string{
   415→		time.RFC3339,
   416→		"2006-01-02T15:04:05",
   417→		"2006-01-02 15:04:05",
   418→		"2006-01-02",
   419→		"01/02/2006",
   420→		"02-01-2006",
   421→	} {
   422→		if _, err := time.Parse(layout, s); err == nil {
   423→			return TypeTimestamp
   424→		}
   425→	}
   426→
   427→	return TypeString
   428→}
   429→
   430→// buildArrowSchema creates Arrow schema from headers and types.
   431→func (r *RobustPath) buildArrowSchema(headers [][]byte, types []ColumnType) *arrow.Schema {
   432→	fields := make([]arrow.Field, len(headers))
   433→
   434→	for i, header := range headers {
   435→		name := string(header)
   436→		var dataType arrow.DataType
   437→
   438→		switch types[i] {
   439→		case TypeInt64:
   440→			dataType = arrow.PrimitiveTypes.Int64
   441→		case TypeFloat64:
   442→			dataType = arrow.PrimitiveTypes.Float64
   443→		case TypeBool:
   444→			dataType = arrow.FixedWidthTypes.Boolean
   445→		case TypeTimestamp:
   446→			dataType = &arrow.TimestampType{Unit: arrow.Microsecond}
   447→		default:
   448→			dataType = arrow.BinaryTypes.String
   449→		}
   450→
   451→		fields[i] = arrow.Field{Name: name, Type: dataType, Nullable: true}
   452→	}
   453→
   454→	return arrow.NewSchema(fields, nil)
   455→}
   456→
   457→// createBuilders creates Arrow builders for each column.
   458→func (r *RobustPath) createBuilders(schema *arrow.Schema, alloc memory.Allocator) []array.Builder {
   459→	builders := make([]array.Builder, schema.NumFields())
   460→
   461→	for i, field := range schema.Fields() {
   462→		switch field.Type.ID() {
   463→		case arrow.INT64:
   464→			builders[i] = array.NewInt64Builder(alloc)
   465→		case arrow.FLOAT64:
   466→			builders[i] = array.NewFloat64Builder(alloc)
   467→		case arrow.BOOL:
   468→			builders[i] = array.NewBooleanBuilder(alloc)
   469→		case arrow.TIMESTAMP:
   470→			builders[i] = array.NewTimestampBuilder(alloc, &arrow.TimestampType{Unit: arrow.Microsecond})
   471→		default:
   472→			builders[i] = array.NewStringBuilder(alloc)
   473→		}
   474→	}
   475→
   476→	return builders
   477→}
   478→
   479→// appendRecord adds a record to builders.
   480→func (r *RobustPath) appendRecord(builders []array.Builder, types []ColumnType, fields [][]byte, headers [][]byte) {
   481→	for i, builder := range builders {
   482→		var value []byte
   483→		if i < len(fields) {
   484→			value = fields[i]
   485→		}
   486→
   487→		if len(value) == 0 || r.isNullValue(value) {
   488→			builder.AppendNull()
   489→			continue
   490→		}
   491→
   492→		s := string(bytes.TrimSpace(value))
   493→
   494→		switch types[i] {
   495→		case TypeInt64:
   496→			if v, err := strconv.ParseInt(s, 10, 64); err == nil {
   497→				builder.(*array.Int64Builder).Append(v)
   498→			} else {
   499→				builder.AppendNull()
   500→			}
   501→		case TypeFloat64:
   502→			if v, err := strconv.ParseFloat(s, 64); err == nil {
   503→				builder.(*array.Float64Builder).Append(v)
   504→			} else {
   505→				builder.AppendNull()
   506→			}
   507→		case TypeBool:
   508→			switch s {
   509→			case "true", "True", "TRUE", "1", "yes":
   510→				builder.(*array.BooleanBuilder).Append(true)
   511→			case "false", "False", "FALSE", "0", "no":
   512→				builder.(*array.BooleanBuilder).Append(false)
   513→			default:
   514→				builder.AppendNull()
   515→			}
   516→		case TypeTimestamp:
   517→			if t := r.parseTimestamp(s); !t.IsZero() {
   518→				builder.(*array.TimestampBuilder).Append(arrow.Timestamp(t.UnixMicro()))
   519→			} else {
   520→				builder.AppendNull()
   521→			}
   522→		default:
   523→			builder.(*array.StringBuilder).Append(s)
   524→		}
   525→	}
   526→}
   527→
   528→// isNullValue checks if value represents null.
   529→func (r *RobustPath) isNullValue(value []byte) bool {
   530→	s := string(bytes.TrimSpace(value))
   531→	switch s {
   532→	case "", "NULL", "null", "NA", "N/A", "n/a", "None", "none", "nil", "-", "\\N":
   533→		return true
   534→	}
   535→	return false
   536→}
   537→
   538→// parseTimestamp attempts to parse a timestamp string.
   539→func (r *RobustPath) parseTimestamp(s string) time.Time {
   540→	layouts := []string{
   541→		time.RFC3339,
   542→		time.RFC3339Nano,
   543→		"2006-01-02T15:04:05",
   544→		"2006-01-02 15:04:05",
   545→		"2006-01-02T15:04:05.000",
   546→		"2006-01-02 15:04:05.000",
   547→		"2006-01-02",
   548→		"01/02/2006",
   549→		"02-01-2006",
   550→		"01/02/2006 15:04:05",
   551→		"02-01-2006 15:04:05",
   552→	}
   553→
   554→	for _, layout := range layouts {
   555→		if t, err := time.Parse(layout, s); err == nil {
   556→			return t
   557→		}
   558→	}
   559→	return time.Time{}
   560→}
   561→
   562→// flushBatch writes accumulated data to Parquet.
   563→func (r *RobustPath) flushBatch(writer *pqarrow.FileWriter, schema *arrow.Schema, builders []array.Builder) error {
   564→	// Build arrays
   565→	arrays := make([]arrow.Array, len(builders))
   566→	for i, builder := range builders {
   567→		arrays[i] = builder.NewArray()
   568→		defer arrays[i].Release()
   569→	}
   570→
   571→	// Create record batch
   572→	record := array.NewRecord(schema, arrays, int64(arrays[0].Len()))
   573→	defer record.Release()
   574→
   575→	// Write
   576→	return writer.Write(record)
   577→}
   578→
   579→// buildWriterProps creates Parquet writer properties.
   580→func (r *RobustPath) buildWriterProps(opts Options) *parquet.WriterProperties {
   581→	compression := compress.Codecs.Snappy
   582→	switch opts.Compression {
   583→	case "zstd":
   584→		compression = compress.Codecs.Zstd
   585→	case "gzip":
   586→		compression = compress.Codecs.Gzip
   587→	case "lz4":
   588→		compression = compress.Codecs.Lz4
   589→	case "none", "uncompressed":
   590→		compression = compress.Codecs.Uncompressed
   591→	}
   592→
   593→	rowGroupSize := int64(opts.RowGroupSize)
   594→	if rowGroupSize <= 0 {
   595→		rowGroupSize = 10000
   596→	}
   597→
   598→	return parquet.NewWriterProperties(
   599→		parquet.WithCompression(compression),
   600→		parquet.WithDictionaryDefault(true),
   601→		parquet.WithDataPageSize(1024*1024), // 1MB
   602→	)
   603→}
   604→
   605→// processJSON handles JSON/JSONL files.
   606→func (r *RobustPath) processJSON(ctx context.Context, inputPath, outputPath string, analysis *FileAnalysis, opts Options) (*Result, error) {
   607→	f, err := os.Open(inputPath)
   608→	if err != nil {
   609→		return nil, fmt.Errorf("failed to open input: %w", err)
   610→	}
   611→	defer f.Close()
   612→
   613→	inputInfo, _ := f.Stat()
   614→
   615→	// Detect if JSONL or JSON array
   616→	isJSONL := analysis.Format == FormatJSONL
   617→
   618→	var records []map[string]interface{}
   619→	var rowCount int64
   620→
   621→	if isJSONL {
   622→		// Process line by line
   623→		scanner := bufio.NewScanner(f)
   624→		scanner.Buffer(make([]byte, 1024*1024), 32*1024*1024) // 32MB max line
   625→
   626→		for scanner.Scan() {
   627→			select {
   628→			case <-ctx.Done():
   629→				return nil, ctx.Err()
   630→			default:
   631→			}
   632→
   633→			line := scanner.Bytes()
   634→			if len(bytes.TrimSpace(line)) == 0 {
   635→				continue
   636→			}
   637→
   638→			var record map[string]interface{}
   639→			if err := json.Unmarshal(line, &record); err != nil {
   640→				continue // Skip malformed lines
   641→			}
   642→			records = append(records, record)
   643→			rowCount++
   644→		}
   645→	} else {
   646→		// Parse as JSON array
   647→		var arr []map[string]interface{}
   648→		decoder := json.NewDecoder(f)
   649→		if err := decoder.Decode(&arr); err != nil {
   650→			return nil, fmt.Errorf("failed to parse JSON: %w", err)
   651→		}
   652→		records = arr
   653→		rowCount = int64(len(arr))
   654→	}
   655→
   656→	if len(records) == 0 {
   657→		return nil, fmt.Errorf("no records found")
   658→	}
   659→
   660→	// Build schema from first few records
   661→	schema := r.buildJSONSchema(records[:min(100, len(records))])
   662→
   663→	// Write to Parquet
   664→	outFile, err := os.Create(outputPath)
   665→	if err != nil {
   666→		return nil, fmt.Errorf("failed to create output: %w", err)
   667→	}
   668→	defer outFile.Close()
   669→
   670→	writerProps := r.buildWriterProps(opts)
   671→	arrowProps := pqarrow.NewArrowWriterProperties(pqarrow.WithAllocator(memory.DefaultAllocator))
   672→
   673→	writer, err := pqarrow.NewFileWriter(schema, outFile, writerProps, arrowProps)
   674→	if err != nil {
   675→		return nil, fmt.Errorf("failed to create writer: %w", err)
   676→	}
   677→	defer writer.Close()
   678→
   679→	// Build and write batches
   680→	batchSize := 8192
   681→	builders := r.createStringBuilders(schema, memory.DefaultAllocator)
   682→
   683→	for i, record := range records {
   684→		r.appendJSONRecord(builders, schema, record)
   685→
   686→		if (i+1)%batchSize == 0 {
   687→			if err := r.flushStringBatch(writer, schema, builders); err != nil {
   688→				return nil, err
   689→			}
   690→		}
   691→	}
   692→
   693→	// Flush remaining
   694→	if len(records)%batchSize != 0 {
   695→		if err := r.flushStringBatch(writer, schema, builders); err != nil {
   696→			return nil, err
   697→		}
   698→	}
   699→
   700→	outputInfo, _ := os.Stat(outputPath)
   701→	outputSize := int64(0)
   702→	if outputInfo != nil {
   703→		outputSize = outputInfo.Size()
   704→	}
   705→
   706→	return &Result{
   707→		InputPath:       inputPath,
   708→		OutputPath:      outputPath,
   709→		Format:          analysis.Format,
   710→		Strategy:        StrategyRobustGo,
   711→		RowCount:        rowCount,
   712→		ColumnCount:     schema.NumFields(),
   713→		InputSize:       inputInfo.Size(),
   714→		OutputSize:      outputSize,
   715→		CompressionRate: float64(inputInfo.Size()) / float64(outputSize),
   716→	}, nil
   717→}
   718→
   719→// buildJSONSchema creates schema from JSON records.
   720→func (r *RobustPath) buildJSONSchema(records []map[string]interface{}) *arrow.Schema {
   721→	// Collect all keys
   722→	keys := make(map[string]bool)
   723→	for _, record := range records {
   724→		for k := range record {
   725→			keys[k] = true
   726→		}
   727→	}
   728→
   729→	// Build schema (all strings for simplicity)
   730→	fields := make([]arrow.Field, 0, len(keys))
   731→	for k := range keys {
   732→		fields = append(fields, arrow.Field{Name: k, Type: arrow.BinaryTypes.String, Nullable: true})
   733→	}
   734→
   735→	return arrow.NewSchema(fields, nil)
   736→}
   737→
   738→// createStringBuilders creates string builders for all columns.
   739→func (r *RobustPath) createStringBuilders(schema *arrow.Schema, alloc memory.Allocator) []*array.StringBuilder {
   740→	builders := make([]*array.StringBuilder, schema.NumFields())
   741→	for i := range builders {
   742→		builders[i] = array.NewStringBuilder(alloc)
   743→	}
   744→	return builders
   745→}
   746→
   747→// appendJSONRecord adds a JSON record to builders.
   748→func (r *RobustPath) appendJSONRecord(builders []*array.StringBuilder, schema *arrow.Schema, record map[string]interface{}) {
   749→	for i, field := range schema.Fields() {
   750→		if val, ok := record[field.Name]; ok && val != nil {
   751→			builders[i].Append(fmt.Sprintf("%v", val))
   752→		} else {
   753→			builders[i].AppendNull()
   754→		}
   755→	}
   756→}
   757→
   758→// flushStringBatch writes string data to Parquet.
   759→func (r *RobustPath) flushStringBatch(writer *pqarrow.FileWriter, schema *arrow.Schema, builders []*array.StringBuilder) error {
   760→	arrays := make([]arrow.Array, len(builders))
   761→	for i, builder := range builders {
   762→		arrays[i] = builder.NewArray()
   763→		defer arrays[i].Release()
   764→	}
   765→
   766→	record := array.NewRecord(schema, arrays, int64(arrays[0].Len()))
   767→	defer record.Release()
   768→
   769→	return writer.Write(record)
   770→}
   771→
   772→// processXES handles XES process mining files with streaming.
   773→func (r *RobustPath) processXES(ctx context.Context, inputPath, outputPath string, analysis *FileAnalysis, opts Options) (*Result, error) {
   774→	// Use the existing XES parser but with robust error handling
   775→	// For now, delegate to a simplified implementation
   776→	return nil, fmt.Errorf("XES processing via robust path not yet implemented - use fast path")
   777→}
   778→
   779→// processGzip handles gzip-compressed files.
   780→func (r *RobustPath) processGzip(ctx context.Context, inputPath, outputPath string, analysis *FileAnalysis, opts Options) (*Result, error) {
   781→	f, err := os.Open(inputPath)
   782→	if err != nil {
   783→		return nil, err
   784→	}
   785→	defer f.Close()
   786→
   787→	gz, err := gzip.NewReader(f)
   788→	if err != nil {
   789→		return nil, fmt.Errorf("failed to create gzip reader: %w", err)
   790→	}
   791→	defer gz.Close()
   792→
   793→	// Create temp file for decompressed data
   794→	tmpFile, err := os.CreateTemp("", "ingest-*.tmp")
   795→	if err != nil {
   796→		return nil, fmt.Errorf("failed to create temp file: %w", err)
   797→	}
   798→	tmpPath := tmpFile.Name()
   799→	defer os.Remove(tmpPath)
   800→
   801→	// Decompress
   802→	if _, err := io.Copy(tmpFile, gz); err != nil {
   803→		tmpFile.Close()
   804→		return nil, fmt.Errorf("decompression failed: %w", err)
   805→	}
   806→	tmpFile.Close()
   807→
   808→	// Analyze and process the decompressed file
   809→	innerAnalysis, err := NewDetector().Analyze(tmpPath)
   810→	if err != nil {
   811→		return nil, fmt.Errorf("failed to analyze decompressed file: %w", err)
   812→	}
   813→
   814→	return r.Process(ctx, tmpPath, outputPath, innerAnalysis, opts)
   815→}
   816→
   817→// StreamingStats tracks progress during streaming processing.
   818→type StreamingStats struct {
   819→	BytesRead    atomic.Int64
   820→	RowsWritten  atomic.Int64
   821→	ErrorCount   atomic.Int64
   822→	RecoverCount atomic.Int64
   823→}
   824→
   825→func min(a, b int) int {
   826→	if a < b {
   827→		return a
   828→	}
   829→	return b
   830→}
   831→

<system-reminder>
Whenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.
</system-reminder>

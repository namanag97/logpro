     1→package ingest
     2→
     3→import (
     4→	"context"
     5→	"database/sql"
     6→	"fmt"
     7→	"os"
     8→	"path/filepath"
     9→	"runtime"
    10→	"strings"
    11→
    12→	_ "github.com/marcboeker/go-duckdb"
    13→)
    14→
    15→// FastPath uses DuckDB native readers for maximum performance.
    16→// This path is used for clean files where DuckDB can handle parsing.
    17→type FastPath struct {
    18→	db *sql.DB
    19→}
    20→
    21→// NewFastPath creates a new DuckDB-based fast path.
    22→func NewFastPath() (*FastPath, error) {
    23→	db, err := sql.Open("duckdb", "")
    24→	if err != nil {
    25→		return nil, fmt.Errorf("failed to initialize DuckDB: %w", err)
    26→	}
    27→
    28→	return &FastPath{db: db}, nil
    29→}
    30→
    31→// Close releases DuckDB resources.
    32→func (f *FastPath) Close() error {
    33→	return f.db.Close()
    34→}
    35→
    36→// Process handles file conversion using DuckDB native readers.
    37→func (f *FastPath) Process(ctx context.Context, inputPath, outputPath string, analysis *FileAnalysis, opts Options) (*Result, error) {
    38→	// Configure DuckDB threads
    39→	threads := opts.DuckDBThreads
    40→	if threads <= 0 {
    41→		threads = runtime.NumCPU()
    42→	}
    43→	if _, err := f.db.ExecContext(ctx, fmt.Sprintf("SET threads=%d", threads)); err != nil {
    44→		// Non-fatal, continue with default
    45→	}
    46→
    47→	// For large files, disable insertion order preservation
    48→	if analysis.Size > 1024*1024*1024 { // 1GB
    49→		f.db.ExecContext(ctx, "SET preserve_insertion_order=false")
    50→	}
    51→
    52→	// Ensure output directory exists
    53→	if err := os.MkdirAll(filepath.Dir(outputPath), 0755); err != nil {
    54→		return nil, fmt.Errorf("failed to create output directory: %w", err)
    55→	}
    56→
    57→	var rowCount int64
    58→	var columnCount int
    59→	var err error
    60→
    61→	switch analysis.Format {
    62→	case FormatCSV, FormatTSV:
    63→		rowCount, columnCount, err = f.processCSV(ctx, inputPath, outputPath, analysis, opts)
    64→	case FormatJSON:
    65→		rowCount, columnCount, err = f.processJSON(ctx, inputPath, outputPath, false, opts)
    66→	case FormatJSONL:
    67→		rowCount, columnCount, err = f.processJSON(ctx, inputPath, outputPath, true, opts)
    68→	case FormatParquet:
    69→		rowCount, columnCount, err = f.processParquet(ctx, inputPath, outputPath, opts)
    70→	case FormatGzip:
    71→		rowCount, columnCount, err = f.processGzip(ctx, inputPath, outputPath, analysis, opts)
    72→	default:
    73→		return nil, fmt.Errorf("format %s not supported by fast path", analysis.Format)
    74→	}
    75→
    76→	if err != nil {
    77→		return nil, err
    78→	}
    79→
    80→	// Get output size
    81→	outputInfo, err := os.Stat(outputPath)
    82→	if err != nil {
    83→		return nil, fmt.Errorf("failed to stat output: %w", err)
    84→	}
    85→
    86→	result := &Result{
    87→		InputPath:       inputPath,
    88→		OutputPath:      outputPath,
    89→		Format:          analysis.Format,
    90→		Strategy:        StrategyFastDuckDB,
    91→		RowCount:        rowCount,
    92→		ColumnCount:     columnCount,
    93→		InputSize:       analysis.Size,
    94→		OutputSize:      outputInfo.Size(),
    95→		CompressionRate: float64(analysis.Size) / float64(outputInfo.Size()),
    96→	}
    97→
    98→	return result, nil
    99→}
   100→
   101→// processCSV converts CSV to Parquet using DuckDB read_csv_auto.
   102→func (f *FastPath) processCSV(ctx context.Context, inputPath, outputPath string, analysis *FileAnalysis, opts Options) (int64, int, error) {
   103→	// Build read_csv options based on analysis
   104→	csvOpts := f.buildCSVOptions(analysis, opts)
   105→
   106→	// Get row group size
   107→	rowGroupSize := opts.RowGroupSize
   108→	if rowGroupSize <= 0 {
   109→		rowGroupSize = 10000 // Optimized default
   110→	}
   111→
   112→	// Get compression
   113→	compression := opts.Compression
   114→	if compression == "" {
   115→		compression = "snappy"
   116→	}
   117→
   118→	// Build query
   119→	query := fmt.Sprintf(`
   120→		COPY (SELECT * FROM read_csv_auto('%s'%s))
   121→		TO '%s' (FORMAT PARQUET, COMPRESSION '%s', ROW_GROUP_SIZE %d)
   122→	`, escapePath(inputPath), csvOpts, escapePath(outputPath), compression, rowGroupSize)
   123→
   124→	if _, err := f.db.ExecContext(ctx, query); err != nil {
   125→		return 0, 0, fmt.Errorf("CSV conversion failed: %w", err)
   126→	}
   127→
   128→	return f.getParquetStats(outputPath)
   129→}
   130→
   131→// buildCSVOptions constructs DuckDB read_csv options based on analysis.
   132→func (f *FastPath) buildCSVOptions(analysis *FileAnalysis, opts Options) string {
   133→	var options []string
   134→
   135→	// Header
   136→	options = append(options, fmt.Sprintf("header=%v", analysis.HasHeader))
   137→
   138→	// Delimiter
   139→	delimStr := string(analysis.Delimiter)
   140→	if analysis.Delimiter == '\t' {
   141→		delimStr = "\\t"
   142→	}
   143→	options = append(options, fmt.Sprintf("delim='%s'", delimStr))
   144→
   145→	// Quote character
   146→	if analysis.HasQuotedFields {
   147→		options = append(options, fmt.Sprintf("quote='%c'", analysis.QuoteChar))
   148→	}
   149→
   150→	// Parallel reading for clean files
   151→	if analysis.IsClean && !analysis.HasEmbeddedNewlines {
   152→		options = append(options, "parallel=true")
   153→	}
   154→
   155→	// Type inference
   156→	options = append(options, "all_varchar=false")
   157→
   158→	// Null values
   159→	if len(analysis.NullValueStrings) > 0 {
   160→		nulls := make([]string, len(analysis.NullValueStrings))
   161→		for i, s := range analysis.NullValueStrings {
   162→			nulls[i] = fmt.Sprintf("'%s'", strings.ReplaceAll(s, "'", "''"))
   163→		}
   164→		options = append(options, fmt.Sprintf("null_padding=true"))
   165→	}
   166→
   167→	// Sample size for type inference
   168→	options = append(options, "sample_size=10000")
   169→
   170→	if len(options) == 0 {
   171→		return ""
   172→	}
   173→	return ", " + strings.Join(options, ", ")
   174→}
   175→
   176→// processJSON converts JSON/JSONL to Parquet.
   177→func (f *FastPath) processJSON(ctx context.Context, inputPath, outputPath string, isLines bool, opts Options) (int64, int, error) {
   178→	format := "auto"
   179→	if isLines {
   180→		format = "newline_delimited"
   181→	}
   182→
   183→	rowGroupSize := opts.RowGroupSize
   184→	if rowGroupSize <= 0 {
   185→		rowGroupSize = 10000
   186→	}
   187→
   188→	compression := opts.Compression
   189→	if compression == "" {
   190→		compression = "snappy"
   191→	}
   192→
   193→	query := fmt.Sprintf(`
   194→		COPY (SELECT * FROM read_json_auto('%s', format='%s', maximum_object_size=33554432))
   195→		TO '%s' (FORMAT PARQUET, COMPRESSION '%s', ROW_GROUP_SIZE %d)
   196→	`, escapePath(inputPath), format, escapePath(outputPath), compression, rowGroupSize)
   197→
   198→	if _, err := f.db.ExecContext(ctx, query); err != nil {
   199→		return 0, 0, fmt.Errorf("JSON conversion failed: %w", err)
   200→	}
   201→
   202→	return f.getParquetStats(outputPath)
   203→}
   204→
   205→// processParquet re-compresses or copies Parquet files.
   206→func (f *FastPath) processParquet(ctx context.Context, inputPath, outputPath string, opts Options) (int64, int, error) {
   207→	rowGroupSize := opts.RowGroupSize
   208→	if rowGroupSize <= 0 {
   209→		rowGroupSize = 10000
   210→	}
   211→
   212→	compression := opts.Compression
   213→	if compression == "" {
   214→		compression = "snappy"
   215→	}
   216→
   217→	query := fmt.Sprintf(`
   218→		COPY (SELECT * FROM read_parquet('%s'))
   219→		TO '%s' (FORMAT PARQUET, COMPRESSION '%s', ROW_GROUP_SIZE %d)
   220→	`, escapePath(inputPath), escapePath(outputPath), compression, rowGroupSize)
   221→
   222→	if _, err := f.db.ExecContext(ctx, query); err != nil {
   223→		return 0, 0, fmt.Errorf("Parquet conversion failed: %w", err)
   224→	}
   225→
   226→	return f.getParquetStats(outputPath)
   227→}
   228→
   229→// processGzip handles gzip-compressed files.
   230→func (f *FastPath) processGzip(ctx context.Context, inputPath, outputPath string, analysis *FileAnalysis, opts Options) (int64, int, error) {
   231→	// DuckDB can read gzipped files directly
   232→	// Determine the underlying format from the filename
   233→	baseName := strings.TrimSuffix(inputPath, ".gz")
   234→	ext := strings.ToLower(filepath.Ext(baseName))
   235→
   236→	var format Format
   237→	switch ext {
   238→	case ".csv":
   239→		format = FormatCSV
   240→	case ".tsv":
   241→		format = FormatTSV
   242→	case ".json":
   243→		format = FormatJSON
   244→	case ".jsonl", ".ndjson":
   245→		format = FormatJSONL
   246→	default:
   247→		format = FormatCSV // Default to CSV
   248→	}
   249→
   250→	// Create a modified analysis for the underlying format
   251→	innerAnalysis := *analysis
   252→	innerAnalysis.Format = format
   253→
   254→	switch format {
   255→	case FormatCSV, FormatTSV:
   256→		return f.processCSV(ctx, inputPath, outputPath, &innerAnalysis, opts)
   257→	case FormatJSON:
   258→		return f.processJSON(ctx, inputPath, outputPath, false, opts)
   259→	case FormatJSONL:
   260→		return f.processJSON(ctx, inputPath, outputPath, true, opts)
   261→	default:
   262→		return f.processCSV(ctx, inputPath, outputPath, &innerAnalysis, opts)
   263→	}
   264→}
   265→
   266→// getParquetStats retrieves row count and column count from Parquet metadata.
   267→func (f *FastPath) getParquetStats(path string) (int64, int, error) {
   268→	// Get row count from metadata (fast, doesn't scan data)
   269→	var rowCount int64
   270→	metaQuery := fmt.Sprintf(`SELECT SUM(num_rows) FROM parquet_metadata('%s')`, escapePath(path))
   271→	if err := f.db.QueryRow(metaQuery).Scan(&rowCount); err != nil {
   272→		// Fallback to COUNT(*)
   273→		countQuery := fmt.Sprintf(`SELECT COUNT(*) FROM read_parquet('%s')`, escapePath(path))
   274→		if err := f.db.QueryRow(countQuery).Scan(&rowCount); err != nil {
   275→			return 0, 0, fmt.Errorf("failed to get row count: %w", err)
   276→		}
   277→	}
   278→
   279→	// Get column count from schema
   280→	var columnCount int
   281→	schemaQuery := fmt.Sprintf(`SELECT COUNT(*) FROM parquet_schema('%s') WHERE name IS NOT NULL`, escapePath(path))
   282→	if err := f.db.QueryRow(schemaQuery).Scan(&columnCount); err != nil {
   283→		// Fallback to DESCRIBE
   284→		descQuery := fmt.Sprintf(`DESCRIBE SELECT * FROM read_parquet('%s')`, escapePath(path))
   285→		rows, err := f.db.Query(descQuery)
   286→		if err != nil {
   287→			return rowCount, 0, nil
   288→		}
   289→		defer rows.Close()
   290→		for rows.Next() {
   291→			columnCount++
   292→		}
   293→	}
   294→
   295→	return rowCount, columnCount, nil
   296→}
   297→
   298→// BulkCSV processes multiple CSV files efficiently.
   299→func (f *FastPath) BulkCSV(ctx context.Context, inputGlob, outputPath string, opts Options) (*Result, error) {
   300→	rowGroupSize := opts.RowGroupSize
   301→	if rowGroupSize <= 0 {
   302→		rowGroupSize = 10000
   303→	}
   304→
   305→	compression := opts.Compression
   306→	if compression == "" {
   307→		compression = "snappy"
   308→	}
   309→
   310→	// Use glob pattern for multiple files
   311→	query := fmt.Sprintf(`
   312→		COPY (SELECT * FROM read_csv_auto('%s', header=true, union_by_name=true))
   313→		TO '%s' (FORMAT PARQUET, COMPRESSION '%s', ROW_GROUP_SIZE %d)
   314→	`, escapePath(inputGlob), escapePath(outputPath), compression, rowGroupSize)
   315→
   316→	if _, err := f.db.ExecContext(ctx, query); err != nil {
   317→		return nil, fmt.Errorf("bulk CSV conversion failed: %w", err)
   318→	}
   319→
   320→	rowCount, columnCount, err := f.getParquetStats(outputPath)
   321→	if err != nil {
   322→		return nil, err
   323→	}
   324→
   325→	outputInfo, _ := os.Stat(outputPath)
   326→	outputSize := int64(0)
   327→	if outputInfo != nil {
   328→		outputSize = outputInfo.Size()
   329→	}
   330→
   331→	return &Result{
   332→		InputPath:   inputGlob,
   333→		OutputPath:  outputPath,
   334→		Format:      FormatCSV,
   335→		Strategy:    StrategyFastDuckDB,
   336→		RowCount:    rowCount,
   337→		ColumnCount: columnCount,
   338→		OutputSize:  outputSize,
   339→	}, nil
   340→}
   341→
   342→// Query executes a custom DuckDB query for advanced use cases.
   343→func (f *FastPath) Query(ctx context.Context, query string) (*sql.Rows, error) {
   344→	return f.db.QueryContext(ctx, query)
   345→}
   346→
   347→// Exec executes a custom DuckDB statement.
   348→func (f *FastPath) Exec(ctx context.Context, query string) (sql.Result, error) {
   349→	return f.db.ExecContext(ctx, query)
   350→}
   351→
   352→// escapePath escapes a path for use in SQL.
   353→func escapePath(path string) string {
   354→	return strings.ReplaceAll(path, "'", "''")
   355→}
   356→

<system-reminder>
Whenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.
</system-reminder>

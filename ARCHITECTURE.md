# LogFlow Architecture

## Module Map

```
logflow/
â”œâ”€â”€ cmd/                    # CLI entry points
â”‚   â””â”€â”€ logflow/           # Main CLI application
â”‚
â”œâ”€â”€ internal/              # Private implementation details
â”‚   â””â”€â”€ model/             # Core event model (Event, CaseID, Activity)
â”‚
â””â”€â”€ pkg/                   # Public, reusable packages
    â”‚
    â”œâ”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CORE LAYER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â”‚
    â”œâ”€â”€ pipeline/          # Event processing pipeline
    â”‚   â”œâ”€â”€ orchestrator_v2.go   # Production pipeline coordinator
    â”‚   â”œâ”€â”€ enterprise.go        # Enterprise wrapper (telemetry, resilience)
    â”‚   â”œâ”€â”€ dlq.go               # Dead letter queue for failed records
    â”‚   â””â”€â”€ interfaces.go        # Source, Sink, Processor interfaces
    â”‚
    â”œâ”€â”€ sources/           # Data input adapters
    â”‚   â”œâ”€â”€ csv.go              # CSV file reader
    â”‚   â”œâ”€â”€ json.go             # JSON/JSONL reader
    â”‚   â””â”€â”€ xes.go              # XES (process mining standard) reader
    â”‚
    â”œâ”€â”€ sinks/             # Data output adapters
    â”‚   â””â”€â”€ parquet.go          # Parquet file writer
    â”‚
    â”œâ”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ STORAGE LAYER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â”‚
    â”œâ”€â”€ storage/           # Storage abstraction
    â”‚   â”œâ”€â”€ cloud.go            # Unified storage interface
    â”‚   â””â”€â”€ s3/                 # AWS S3 implementation
    â”‚       â”œâ”€â”€ s3.go           # Full S3 client
    â”‚       â””â”€â”€ select.go       # S3 Select pushdown queries
    â”‚
    â”œâ”€â”€ checkpoint/        # Resume capability
    â”‚   â”œâ”€â”€ checkpoint.go       # Local checkpoint manager
    â”‚   â”œâ”€â”€ backends.go         # Backend interface
    â”‚   â”œâ”€â”€ s3.go               # S3 checkpoint backend
    â”‚   â””â”€â”€ redis.go            # Redis checkpoint backend
    â”‚
    â”œâ”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PROCESS MINING LAYER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â”‚
    â”œâ”€â”€ pmpt/              # Process Merkle Patricia Tree (Case-Centric)
    â”‚   â”œâ”€â”€ tree.go             # Core tree structure (O(1) comparison)
    â”‚   â”œâ”€â”€ interval.go         # Interval tree for time queries
    â”‚   â”œâ”€â”€ builder.go          # Incremental tree construction
    â”‚   â””â”€â”€ hybrid.go           # Combined sequence + time queries
    â”‚
    â”œâ”€â”€ ocel/              # Object-Centric Event Logs (OCEL 2.0)
    â”‚   â”œâ”€â”€ model.go            # OCEL 2.0 data types
    â”‚   â”œâ”€â”€ store.go            # DuckDB relational storage
    â”‚   â”œâ”€â”€ import.go           # Import from CSV/JSON/XML
    â”‚   â”œâ”€â”€ export.go           # Export to OCEL standard formats
    â”‚   â””â”€â”€ discovery.go        # OC-DFG discovery algorithm
    â”‚
    â”œâ”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ QUALITY LAYER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â”‚
    â”œâ”€â”€ validation/        # Data validation
    â”‚   â””â”€â”€ quality/            # Quality rules engine
    â”‚       â””â”€â”€ rules.go        # Configurable validation rules
    â”‚
    â”œâ”€â”€ parser/            # Parsing utilities
    â”‚   â””â”€â”€ healing/            # Self-healing parser
    â”‚       â”œâ”€â”€ rules.go        # Fix rules (encoding, quoting, etc.)
    â”‚       â”œâ”€â”€ detector.go     # Error pattern detection
    â”‚       â””â”€â”€ fixer.go        # Auto-repair wrapper
    â”‚
    â”œâ”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ OPERATIONS LAYER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â”‚
    â”œâ”€â”€ telemetry/         # Observability
    â”‚   â”œâ”€â”€ telemetry.go        # Tracer, Metrics, Spans
    â”‚   â””â”€â”€ otel.go             # OpenTelemetry OTLP export
    â”‚
    â”œâ”€â”€ resilience/        # Fault tolerance
    â”‚   â””â”€â”€ resilience.go       # Circuit breaker, poison pill handler
    â”‚
    â”œâ”€â”€ lifecycle/         # Process lifecycle
    â”‚   â””â”€â”€ shutdown.go         # Graceful shutdown manager
    â”‚
    â””â”€â”€ errors/            # Error handling
        â””â”€â”€ errors.go           # Typed errors with codes
```

---

## Layer Dependencies

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CLI (cmd/)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CORE LAYER                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  Pipeline   â”‚â”€â”€â”‚   Sources   â”‚â”€â”€â”‚    Sinks    â”‚          â”‚
â”‚  â”‚ Orchestratorâ”‚  â”‚  (CSV,JSON) â”‚  â”‚  (Parquet)  â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                   â”‚                    â”‚
         â–¼                   â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   STORAGE LAYER                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   Storage   â”‚  â”‚ Checkpoint  â”‚  â”‚   DuckDB    â”‚          â”‚
â”‚  â”‚ (Local, S3) â”‚  â”‚(Local,S3,Redis)â”‚ â”‚  (OCEL)    â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                   â”‚                    â”‚
         â–¼                   â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                PROCESS MINING LAYER                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚    PMPT     â”‚  â”‚    OCEL     â”‚                           â”‚
â”‚  â”‚(Case-Centric)â”‚ â”‚(Object-Centric)â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                   â”‚
         â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  QUALITY LAYER                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚ Validation  â”‚  â”‚   Healing   â”‚                           â”‚
â”‚  â”‚   Rules     â”‚  â”‚   Parser    â”‚                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 OPERATIONS LAYER                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  Telemetry  â”‚  â”‚ Resilience  â”‚  â”‚  Lifecycle  â”‚          â”‚
â”‚  â”‚   (OTLP)    â”‚  â”‚(CircuitBrkr)â”‚  â”‚ (Shutdown)  â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Module Contracts (Interfaces)

### Pipeline (`pkg/pipeline/`)

```go
// Source reads events from an input
type Source interface {
    Name() string
    Read(ctx context.Context, r io.Reader, out chan<- *Event) error
}

// Sink writes events to an output
type Sink interface {
    Name() string
    Write(ctx context.Context, in <-chan *Event) error
    Close() error
}

// Processor transforms events in a pipeline stage
type Processor interface {
    Name() string
    Process(ctx context.Context, in <-chan *Event, out chan<- *Event) error
}

// Inspector observes events without modifying (read-only)
type Inspector interface {
    Name() string
    Inspect(event *Event)
    Report() interface{}
}
```

### Storage (`pkg/storage/`)

```go
type Storage interface {
    Reader(ctx context.Context, path string) (io.ReadCloser, int64, error)
    Writer(ctx context.Context, path string) (io.WriteCloser, error)
    Stat(ctx context.Context, path string) (*FileInfo, error)
    Scheme() string  // "file", "s3", "gs", "az"
}
```

### Checkpoint (`pkg/checkpoint/`)

```go
type Backend interface {
    Save(ctx context.Context, cp *Checkpoint) error
    Load(ctx context.Context, id string) (*Checkpoint, error)
    Delete(ctx context.Context, id string) error
    List(ctx context.Context, prefix string) ([]*Checkpoint, error)
    Name() string  // "local", "s3", "redis"
}
```

---

## OCEL 2.0 Module (Object-Centric Process Mining)

### Why DuckDB Instead of Parquet Nested Types?

OCEL 2.0 requires:
1. **E2O relations with qualifiers** - ternary relation (event, qualifier, object)
2. **O2O relations** - object-to-object links
3. **Versioned object attributes** - attributes change over time

Parquet nested LIST can store `Event â†’ [ObjectID]` but **cannot express**:
- Qualifiers on relationships ("primary", "resource")
- Object-to-object relations
- Temporal attribute versioning

**DuckDB** provides:
- In-process SQL engine (already a dependency)
- Native Parquet export for analytics
- Relational model matching OCEL 2.0 spec exactly

### OCEL 2.0 Mathematical Model

```
L = (E, O, EA, OA, evtype, time, objtype, eatype, oatype, eaval, oaval, E2O, O2O)

Where:
- E âŠ† ğ•Œ_ev           : Set of events
- O âŠ† ğ•Œ_obj          : Set of objects
- E2O âŠ† E Ã— ğ•Œ_qual Ã— O : Event-to-Object relations (qualified)
- O2O âŠ† O Ã— ğ•Œ_qual Ã— O : Object-to-Object relations (qualified)
```

### DuckDB Schema (OCEL 2.0 Compliant)

```sql
-- Events
CREATE TABLE event (
    event_id    VARCHAR PRIMARY KEY,
    event_type  VARCHAR NOT NULL,
    timestamp   TIMESTAMP NOT NULL
);

-- Objects
CREATE TABLE object (
    object_id   VARCHAR PRIMARY KEY,
    object_type VARCHAR NOT NULL
);

-- Event-to-Object (E2O) with qualifier
CREATE TABLE event_object (
    event_id    VARCHAR,
    object_id   VARCHAR,
    qualifier   VARCHAR,
    PRIMARY KEY (event_id, object_id, qualifier)
);

-- Object-to-Object (O2O) with qualifier
CREATE TABLE object_object (
    source_id   VARCHAR,
    target_id   VARCHAR,
    qualifier   VARCHAR,
    PRIMARY KEY (source_id, target_id, qualifier)
);

-- Object attributes (versioned - attributes change over time)
CREATE TABLE object_attribute (
    object_id   VARCHAR,
    attr_name   VARCHAR,
    attr_value  VARCHAR,
    attr_type   VARCHAR,
    timestamp   TIMESTAMP,
    PRIMARY KEY (object_id, attr_name, timestamp)
);
```

---

## Usage Examples

### 1. Basic Pipeline (Case-Centric)

```go
pipeline := pipeline.NewOrchestratorV2(cfg).
    SetSource(sources.NewCSVSource()).
    AddProcessor(transforms.NewFilterProcessor(filter)).
    SetSink(sinks.NewParquetSink(parquetCfg))

err := pipeline.Run(ctx)
```

### 2. Enterprise Pipeline (Production)

```go
enterprise, _ := pipeline.NewEnterpriseOrchestrator(pipeline.EnterpriseConfig{
    ServiceName:    "logflow",
    OTLPEndpoint:   "localhost:4317",
    CheckpointDir:  "/tmp/checkpoints",
    DLQPath:        "/tmp/dlq",
})

enterprise.SetSource(sources.NewCSVSource()).SetSink(sinks.NewParquetSink(cfg))
enterprise.HandleSignals(ctx)
err := enterprise.Run(ctx)
```

### 3. OCEL 2.0 (Object-Centric)

```go
// Create OCEL store
store, _ := ocel.NewStore("process.ocel.db")

// Import with object mapping
importer := ocel.NewImporter(store)
importer.ImportCSV(ctx, reader, ocel.CSVMapping{
    EventID:   "event_id",
    Activity:  "activity",
    Timestamp: "timestamp",
    Objects: map[string]string{
        "order_id":    "Order",
        "customer_id": "Customer",
    },
})

// Discover Object-Centric DFG
dfg := store.DiscoverOCDFG()

// Project to traditional case-centric view
orderLog := store.ProjectByObjectType("Order")
```

---

## Adding New Modules

| To Add | Implement | Location |
|--------|-----------|----------|
| New data source | `Source` interface | `pkg/sources/` |
| New output format | `Sink` interface | `pkg/sinks/` |
| New transformation | `Processor` interface | `pkg/pipeline/` |
| New storage backend | `Storage` interface | `pkg/storage/` |
| New checkpoint backend | `Backend` interface | `pkg/checkpoint/` |

---

## Universal Event Store API (OCEL + N-Column Ingest)

### Schema Injection (`pkg/ingest/schema/`)

The schema package extends Arrow schema inference with OCEL 2.0 nested column support.

```go
import "github.com/logflow/logflow/pkg/ingest/schema"

// Get the OCEL nested column type: LIST<STRUCT<object_id: STRING, object_type: STRING>>
ocelType := schema.OCELObjectType()

// Inject the ocel_objects column into any inferred schema
enrichedSchema := schema.InjectOCELColumn(existingSchema)

// Detect which columns are likely object references (e.g., "order_id" â†’ type "order")
hints := schema.IdentifyObjectColumns(existingSchema)

// Build a new RecordBatch with ocel_objects populated from identified columns
enrichedBatch, err := schema.EnrichBatchWithOCEL(alloc, batch, hints)
```

**Key types:**

| Type | Purpose |
|------|---------|
| `ObjectColumnHint` | Describes a column identified as an OCEL object reference (field index, object type, role) |
| `ObjectRole` | `ObjectRoleID` (column holds object IDs) or `ObjectRoleType` (column holds type labels) |

**Constants:**

- `OCELObjectsColumn = "ocel_objects"` â€” reserved column name

### Dynamic Parquet Writer (`pkg/ingest/decoders/`, `pkg/ingest/sinks/`)

The CSV decoder's `createBuilders()` handles Arrow nested types:

```go
// These Arrow types are now supported in builder creation:
// arrow.LIST   â†’ array.NewListBuilder(alloc, elemType)
// arrow.STRUCT â†’ array.NewStructBuilder(alloc, structType)
```

The Parquet sink writes OCEL metadata alongside existing `logflow.*` keys:

| Metadata Key | Value | When Written |
|-------------|-------|--------------|
| `ocel:attributes` | Comma-separated list of all column names | Always |
| `ocel:object_types` | Comma-separated object types | When `ocel_objects` column present |
| `ocel:relationships` | Relationship descriptors | When `ocel_objects` column present |

Pass object types via `opts.Metadata["ocel_object_types"]` and relationships via `opts.Metadata["ocel_relationships"]`.

### Attribute Bitmap Indexes (`pkg/index/`)

Roaring bitmap indexes for fast attribute lookups across Arrow RecordBatches.

```go
import "github.com/logflow/logflow/pkg/index"

idx := index.NewAttributeIndex()

// Index Arrow batches (multi-batch files use rowOffset for global positioning)
idx.IndexBatch(batch, 0)
idx.IndexBatch(batch2, uint32(batch.NumRows()))

// Point lookup: which rows have activity == "Submit Order"?
bm := idx.Lookup("activity", "Submit Order")

// Multi-attribute AND: activity == "Submit Order" AND resource == "Alice"
bm := idx.LookupAnd(map[string]string{
    "activity": "Submit Order",
    "resource": "Alice",
})

// Multi-attribute OR
bm := idx.LookupOr(conditions)

// Inspect the index
idx.Columns()                   // []string of indexed column names
idx.Cardinality("activity")     // number of distinct values
idx.DistinctValues("activity")  // all distinct values
idx.RowCount()                  // total rows indexed

// Serialize / deserialize
idx.WriteTo(writer)
idx.ReadFrom(reader)
```

**Design details:**
- Skips `LIST` and `STRUCT` columns (not flat-indexable)
- Dictionary-encoded Arrow columns are resolved through the dictionary automatically
- Thread-safe via `sync.RWMutex` (concurrent reads, exclusive writes)

### DuckDB Parquet Querier (`pkg/ocel/`)

Query OCEL-enriched Parquet files using DuckDB `read_parquet()` and `UNNEST()`.

```go
import "github.com/logflow/logflow/pkg/ocel"

q, err := ocel.NewParquetQuerier()
defer q.Close()

// Query flat columns only
rows, err := q.QueryFlat(ctx, "events.parquet", `activity = 'Submit'`, "case_id", "activity")

// UNNEST ocel_objects to join flat attributes with object references
rows, err := q.QueryWithObjects(ctx, "events.parquet", `obj.object_type = 'Order'`)

// Filter by object type or object ID
rows, err := q.QueryByObjectType(ctx, "events.parquet", "Order")
rows, err := q.QueryByObjectID(ctx, "events.parquet", "ORD-12345")

// Aggregate queries
types, err := q.QueryObjectTypes(ctx, "events.parquet")
counts, err := q.QueryObjectCounts(ctx, "events.parquet") // map[type]count

// Discover an Object-Centric DFG directly from Parquet
dfg, err := q.DiscoverDFGFromParquet(ctx, "events.parquet", "activity", "timestamp")
// dfg.Edges["Order"]["Submit"]["Approve"] = 42
// dfg.ObjectTypes = ["Order", "Customer"]
// dfg.Activities = ["Submit", "Approve", "Ship"]

// Raw SQL with read_parquet() and UNNEST()
rows, err := q.Raw(ctx, `
    SELECT obj.object_type, COUNT(*)
    FROM read_parquet('events.parquet') t,
    UNNEST(t.ocel_objects) AS obj(object_id, object_type)
    GROUP BY obj.object_type
`)
```

### PMPT Object Awareness (`pkg/pmpt/`)

ProcessNode now carries roaring bitmaps for cases and OCEL objects.

```go
import "github.com/logflow/logflow/pkg/pmpt"

// Enable object tracking in the builder
cfg := pmpt.DefaultBuilderConfig()
cfg.IncludeObjects = true
builder := pmpt.NewBuilder(cfg)

// Add events with object references (model.Event.Objects field)
builder.Add(event)

tree := builder.FlushAll()

// Each ProcessNode now has:
node.CaseBitmap                  // *roaring.Bitmap â€” which cases passed through this node
node.ObjectCounts                // map[string]int64 â€” distinct objects per type
node.ObjectBitmap                // map[string]*roaring.Bitmap â€” object IDs per type
```

**New types:**

| Type | Purpose |
|------|---------|
| `OCELNode` | Wraps `ProcessNode` with `ObjectTypes []string` and `ObjectTypeFrequency map[string]float64` |
| `ObjectTrace` | Extends `Trace` with per-step `Objects [][]ObjectRef` |
| `ObjectRef` | `{ObjectID, ObjectType string}` â€” OCEL object reference on a tree node |

**Event model extension (`internal/model/`):**

```go
type Event struct {
    CaseID     []byte
    Activity   []byte
    Timestamp  int64
    Resource   []byte
    Attributes []Attribute
    Objects    []ObjectRef  // NEW: OCEL object references
}

type ObjectRef struct {
    ObjectID   []byte
    ObjectType []byte
}
```

---

## Research References

- [OCEL 2.0 Specification](https://arxiv.org/html/2403.01975v1)
- [Apache Arrow Parquet Nested Types](https://arrow.apache.org/blog/2022/10/17/arrow-parquet-encoding-part-3/)
- [Go Concurrency Patterns: Pipelines](https://go.dev/blog/pipelines)
- [DuckDB Zero-Copy Arrow Integration](https://duckdb.org/2021/12/03/duck-arrow)
